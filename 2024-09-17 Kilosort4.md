---
title: 2024-09-17 Kilosort4
layout: default 
mathjax: true
mathjax: true
tags: #methods
---
Tags: [[Crow project]]
- ðŸ“š [Kilosort4 â€” Kilosort4 0.0.1 documentation](https://kilosort.readthedocs.io/en/latest/index.html) (also helpful discussion on parameters: [here](https://kilosort.readthedocs.io/en/latest/parameters.html))
- YouTube Video: 

### Understanding
Obsidian: [[2024-11-01 Kilosort youtube video]], [[2024-09-17 Whitening data]], [[2024-10-17 Masking vector (spike sorting)]], [[2024-10-17 Matching pursuit]], [[01 Notes/Conceptual Ideas Notes/2024-10-18 Drift correction|2024-10-18 Drift correction]], [[2023-04-24 K-means]]


### Install instructions


1. Clone Github Repo
2. Follow appropriate ReadMe instructions, 


	1. Install anÂ [Anaconda](https://www.anaconda.com/products/distribution)Â distribution of Python. Note you might need to use an anaconda prompt if you did not add anaconda to the path.
	2. Open an anaconda prompt / command prompt which hasÂ `conda`Â forÂ **python 3**Â in the path
	3. Create a new environment withÂ `conda create --name kilosort python=3.9`. Python 3.10 should work as well.
	4. To activate this new environment, runÂ `conda activate kilosort`
	5. To install kilosort and the GUI, runÂ `python -m pip install kilosort[gui]`
	7. Next, if the CPU version of pytorch was installed (will happen on Windows), remove it withÂ `pip uninstall torch`
	8. Then install the GPU version of pytorchÂ `conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia`

3. If Cuda, nvidia drivers not installed yet:

	If step 8 does not work, you need to make sure the NVIDIA driver for your GPU is installed (availableÂ [here](https://www.nvidia.com/Download/index.aspx?lang=en-us)). You may also need to install the CUDA libraries for it, we recommendÂ [CUDA 11.8](https://developer.nvidia.com/cuda-11-8-0-download-archive).
	
	If pytorch installation still fails, follow the instructionsÂ [here](https://pytorch.org/get-started/locally/)Â to determine what version of pytorch to install. The Anaconda install is strongly recommended on Windows, and then choose the CUDA version that is supported by your GPU (newer GPUs may need newer CUDA versions > 10.2). For instance this command will install the 11.8 version on Linux and Windows (note theÂ `torchvision`Â andÂ `torchaudio`Â commands are removed because kilosort doesn't require them):
	
	`conda install pytorch pytorch-cuda=11.8 pynvml -c pytorch -c nvidia`
	
	ThisÂ [video](https://www.youtube.com/watch?v=gsixIQYvj3U)Â has step-by-step installation instructions for NVIDIA drivers and pytorch in Windows (ignore the environment creation step with the .yml file, we have an environment already, to activate it useÂ `conda activate kilosort`).


4. Add missing packages `pip install ipykernel pandas`

5. Then downgrade numpy to be below 2.0.0 ([NumPy 2.0 compatibility Â· Issue #722 Â· MouseLand/Kilosort](https://github.com/MouseLand/Kilosort/issues/722)), `pip install --upgrade numpy<"2.0.0"`



### Pipeline steps

#### Raw data format

- Example filename: 'ZFM-02370_mini.imec0.ap.bin'
- .ap - action potential data, contrast with .lfp
- .bin - binarized data in column format:
	- n_rows = number of samples (each time sample has duration $$\frac{1}{\text{sampling rate}} =\frac{1}{30000} =0.33ms$$)
	- n_cols = number of channels
	- values in cells = since extracellular recordings, presumably in $$\mu V$$

![image](images/Pasted image 20241101150221.png)
- converted .ap.bin file to df for understanding
- duration of recording: num rows/fs = $$\frac{2700000}{300000}= 90s$$ ([[2024-11-01 Sampling rate & number of data points]])

#### Data loading

- âš ï¸ Provided `urlib.request` sometimes has problems, for tutorial can just download manually and add to folder


#### File path management

- âš ï¸ Take the directory in which ``SAVE_PATH`` is (defined by where is stored 'ZFM-02370_mini.imec0.ap.bin'),
- âŒ number of channels has to be specified in settings

```python
# SAVE_PATH.parent - file directory containg the binary file
settings = {'data_dir': SAVE_PATH.parent, 'n_chan_bin': 385}

```


- âš ï¸ Take the root folder of `SAVE_PATH`, which is stored in `settings['data_dir']`, and create folder `kilosort`.
- E.g. if `SAVE_PATH` in `docs\tutorials`, `results_dir` = ``docs\tutorials\kilosort4``
```python
# outputs saved to results_dir
results_dir = Path(settings['data_dir']).joinpath('kilosort4')
```


- âš ï¸ `results_dir` / ``docs\tutorials\kilosort4`` $$\rightarrow$$ where during `run_kilosort` all the output files are stored, 

```python
ops, st, clu, tF, Wall, similar_templates, is_ref, est_contam_rate, kept_spikes = \
    run_kilosort(
        settings=settings, probe_name='neuropixPhase3B1_kilosortChanMap.mat',
        # save_preprocessed_copy=True
        )
```


- âš ï¸ these output files (.tsv, .npy, etc.) can then be retrieved as variables
```python
ops = load_ops(results_dir / 'ops.npy')
camps = pd.read_csv(results_dir / 'cluster_Amplitude.tsv', sep='\t')['Amplitude'].values
contam_pct = pd.read_csv(results_dir / 'cluster_ContamPct.tsv', sep='\t')['ContamPct'].values
chan_map =  np.load(results_dir / 'channel_map.npy')
templates =  np.load(results_dir / 'templates.npy')
chan_best = (templates**2).sum(axis=1).argmax(axis=-1)
chan_best = chan_map[chan_best]
amplitudes = np.load(results_dir / 'amplitudes.npy')
st = np.load(results_dir / 'spike_times.npy')
clu = np.load(results_dir / 'spike_clusters.npy')
firing_rates = np.unique(clu, return_counts=True)[1] * 30000 / st.max()
dshift = ops['dshift']
```


### Pre-processing in kilosort

- â“ kilosort code `artifact_threshold` $$\rightarrow$$ meaning?

| Step | Name                               | Purpose                                                                                                                               | Obsidian backlinks                                                                            |
| ---- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| 1    | Subtract mean                      | Remove the mean across time for each batch (2s)                                                                                       |                                                                                               |
| 2    | Common average referencing (CAR)   | Subtract at each timepoint the median of the signal across all chanels                                                                |                                                                                               |
| 3    | Temporal filtering                 | Something similar to butterworth high-pass fitler (cutoff = 300Hz), but done in frequency domain, see `fft_highpass` as faster on GPU | [[2023-10-30 Low-pass filter & high-pass filter]], [[2024-07-23 Spectral filtering in ephys]] |
| 4    | Channel whitening / Spatial filter | Remove correlations across channels, e.g. spikes from 100â€“1,000 Î¼m away from the probe $$\rightarrow$$ done for nearest 32 channels     | Akseli docs:, separate doc: [[2024-09-17 Whitening data]]                                     |
| 5    | Drift correction                   | ...                                                                                                                                   |                                                                                               |



![image](images/Pasted image 20250307170720.png)

![image](images/Pasted image 20250307170731.png)
![image](images/Pasted image 20250307170743.png)
![image](images/2024-09-17 Kilosort4 2025-03-07.excalidraw)
- Top: Original data: `amplifier.dat`, Bottom: `temp_wh.dat` (pre-processed raw data)
- âš ï¸ in `C:\Users\FM\Documents\Akseli\Code\kilosort_pipeline\kilosort_plots.ipynb`, there is code with which you can save the pre-processed raw data, and then display in Neuroscope as done here

### Output files of `run_kilosort`

- ðŸ“š Text (partially) taken from documentation in ``Kilosort\kilosort\io.py``
- The following files (and some others not used in tutorial) will be saved in `results_dir`

#### Meta data: `ops`

ops.npy : shape N/A
	`ops`
	Dictionary containing a number of state variables saved throughout
	the sorting process (see `run_kilosort`). We recommend loading with
	`kilosort.io.load_ops`.



#### spike_templates 




#### Understanding : `chan_best` for raster plot 

- ðŸ“š See in `def save_to_phy` (in `io.py`) Note that 'template' here does not refer to the universal or learned templates used for spike detection, as it did in some past versions of Kilosort. Instead, it refersto the average spike waveform (after whitening, filtering, and drift correction) for all spikes assigned to each cluster, which are template-like in shape. We use the term 'template' anyway for this section because that is how they are treated in Phy. Elsewhere in the Kilosort4 code, we would refer to these as 'clusters.'

![image](images/Pasted image 20241108094945.png)
![image](images/2024-09-17 Kilosort4 2024-11-01_1.excalidraw)

| variable in kilosort | filename saved for phy          | variable in phy                      | Plot above | Shape                         |
| -------------------- | ------------------------------- | ------------------------------------ | ---------- | ----------------------------- |
| spike_clusters       | 'spike_templates.npy'           | spike_templates/spike_clusters       | right      | (n_spikes,)                   |
| spike_clusters       | 'spike_clusters.npy'            | spike_templates/spike_clusters       | right      | (n_spikes,)                   |
| spike_templates      | 'spike_detection_templates.npy' |                                      | right?     | ???                           |
| templates            | 'templates.npy'                 | ``Bunch(data)`` in `class TraceView` | left?      | (n_templates, nt, n_channels) |



channel_map.npy : shape (n_channels,)
	`chan_map`
	Same as probe['chanMap']. Integer indices into rows of binary file
	that map the data to the contacts listed in the probe file.



spike_clusters.npy : shape (n_spikes,)
	`clu`
	For each spike, integer indicating which template it was assigned to.
- âŒ spike_clusters & spike_templates are the same $$\rightarrow$$ see [[2025-01-24 Phy GUI]]

spike_times.npy : shape (n_spikes,)
	`st`
	Sample index of the waveform peak for each spike.

templates.npy : shape (n_templates, nt, n_channels $$\rightarrow$$  $$i, t, j$$)
	Full time x channels template shapes.
	- e.g. (n_templates, nt, n_channels) = (396, 62, 384)
		- where nt = number of time points in each template (template length: $$5ms$$)



![image](images/Pasted image 20241101221902.png)

- example of 3 different templates, for all nt (x-axis) and all channels (colours)


- âš ï¸ `Templates (n_templates, nt, n_channels)` are averaged waveforms (over $$5ms$$ with 60 nt) for all the neurons detected within the same cluster. `chan_best` is a list indexed by `n_templates` that for each template describes which channel has the most representative sum of detected waveforms similar to the template waveform

- â“ `chan_best = chan_map[chan_best]`




`chan_best`
- shape (n_templates) 

```python
templates = Â np.load(results_dir / 'templates.npy')
chan_best = (templates**2).sum(axis=1).argmax(axis=-1)
```


- $$i$$ - templates, $$t$$ - time points 
- `.sum(axis=1)` $$\rightarrow$$ summing over time points (axis=1)


$$\text{sum\_squared}[i,j] = \sum_{t=1}^{62} (\text{templates}[i,t,j])^2 
$$

- Then apply `.argmax(axis=-1)` $$\rightarrow$$ index of the maximum value along **`axis=-1`**, which corresponds to the channels $$j$$ 

$$\text{chan\_best}[i]=\text{argmax}â€‹(\text{sum\_squared}[i,j])
$$



![image](images/Pasted image 20241218110113.png)
![image](images/2024-09-17 Kilosort4 2024-11-01_3.excalidraw)


- âš ï¸ `clu` is an integer indicating, for each spike, which template the spike is assigned to. By passing `chan_best[clu]`, we get the channel number for each spike, since it's most likely that the spike originated from the channel where its associated template is most likely. 



![image](images/Pasted image 20241218110148.png)



![image](images/2024-09-17 Kilosort4 2024-11-01_0.excalidraw)


#### Firing rate activity

spike_clusters.npy : shape (n_spikes,)
	`clu`
	For each spike, integer indicating which template it was assigned to.


```python
clu = np.load(results_dir / 'spike_clusters.npy')
firing_rates = np.unique(clu, return_counts=True)[1] * 30000 / st.max()
```

- âš ï¸ $$\text{firing\_rate} = \frac{\text{spike count}}{T} \times f_{s}$$ 
- ([[2024-11-01 Sampling rate & number of data points]])

```python
unique_elements, counts = np.unique([1, 2, 2, 2, 4, 4], return_counts=True)
# Output
Unique elements: [1 2 4]
Counts: [1 3 2]
```

`clu`: shape(n_spikes) $$\rightarrow$$ ``firing_rates``: shape(n_templates)

![image](images/Pasted image 20241101234233.png)

#### Amplitude
amplitudes.npy : shape (n_spikes,)
	`amplitudes`
	Per-spike amplitudes, computed as the L2 norm of the PC features
	for each spike.

cluster_Amplitude.tsv : shape (n_templates,)
	`camps`
	Per-template amplitudes, computed as the L2 norm of the template.

- â“ no access to the voltage amplitude???


#### Spike positions

#todo 
- [ ] understand further

spike_position.npy : shape (n_spikes,2)
- ,2 $$\rightarrow$$ xy position $$\rightarrow$$ in Margot code then saved for each `clu` as separate `PosX`, `PosY`
 

- ðŸ“š `C:\Users\FM\anaconda3\envs\kilosort\Lib\site-packages\kilosort\postprocessing.py`: ![image](images/Pasted image 20250305181610.png)

![image](images/Pasted image 20250305180637.png)



#### Contamination percentage: MUA vs Good units 

cluster_ContamPct.tsv : shape (n_clusters,)
	`contam_pct`
	"Contamination rate for each template, computed as fraction of refractory
	period violations relative to expectation based on a Poisson process."


![image](images/2024-09-17 Kilosort4 2024-11-01_4.excalidraw)


|                   | Criterion 1                                                                                            | Criterion 2                                                                                                           |
| ----------------- | ------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |
| **Description**   | ratio of refractory coincidences ($$n_k$$) versus coincidences in other bins (e.g. "shoulders")          | probability $$P_k$$ that $$n_k$$ spikes or less would be observed from a Poisson process with rate $$\lambda_{k}= (2k+1)R$$ |
| **Limitation**    | if firing rate of unit is low $$\rightarrow$$ few refractory coincidences may be observed just by chance |                                                                                                                       |
| **Criteria**      | $$Q_{12}$$ $$\rightarrow  \min(\frac{Q_i}{\text{min}(\text{Q00}, \text{Q01})})$$                           | $$R_{12}$$ $$\rightarrow \min(P_k)$$ for different windows                                                                |
| **ACG threshold** | $$Q_{12} < 0.2$$                                                                                         | $$R_{12} < 0.2$$                                                                                                        |
| **CCG threshold** | $$Q_{12} < 0.25$$                                                                                        | $$R_{12} < 0.05$$                                                                                                       |



- âŒ $$Q_{12}, R_{12}$$ in our current kilosort version were mixed up in terms of variable names $$\rightarrow$$ in a new issue they will update this
- â“ Later double check whether still $$Q$$ corresponds to criterion 1, and $$R$$ corresponds to criterion 2


- ðŸ“š The different thresholds for ACG and CCG have to do with the function of these decisions: for the ACG, we want small contamination rates $$R_{12}$$ because this indicates a well-isolated neuron, whereas for the CCG we want to prevent clusters from being split if their contamination rate $$R_{12}$$ is indicative of a relationship between these two clusters. This is similar for $$Q12$$


- âš ï¸ For computing the CCG, ACG $$\rightarrow$$ Bin size =1ms, window ($$\delta t$$) = $$\pm 0.5s$$
- ðŸ“š Code from: `"C:\Users\FM\anaconda3\envs\kilosort\Lib\site-packages\kilosort\CCG.py"`
![image](images/Pasted image 20250307171018.png)
![image](images/Pasted image 20250307171042.png)

![image](images/Pasted image 20250307171058.png)
![image](images/2024-09-17 Kilosort4 2025-03-05.excalidraw)


#### Principal components 

- `pc_features` $$\rightarrow$$ shape(n_spikes, n_pcs, nearest_chans)
	- pc_features.npy
	- Tensor of pc features as returned by `template_matching.extract`,
	- âŒ Earlier in kilosort code the shape of `tF` is `(n_spikes, nearest_chans, n_pcs)`, but later they swap the last 2 dimensions as Phy expects this ordering
- `pc_features_ind` $$\rightarrow$$ shape(n_clusters, nearest_chans)
	- pc_features_ind.npy
	- Channel indices associated with the data present in tF for each cluster

- â“ weird that they adjust shape of `tF` $$\rightarrow$$ in `class FeatureView` in `phy\cluster\views\feature.py`, they define shape of Bunch(data) as `(n_spikes, n_channels, n_features)`, and use that for constructing the FeatureView plots