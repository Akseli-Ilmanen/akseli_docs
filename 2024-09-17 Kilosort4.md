---
layout: default
mathjax: true
title: 2024-09-17 Kilosort4
tags: #methods
---
Tags:  
- üìö [Kilosort4 ‚Äî Kilosort4 0.0.1 documentation](https://kilosort.readthedocs.io/en/latest/index.html) (also helpful discussion on parameters: [here](https://kilosort.readthedocs.io/en/latest/parameters.html))
- YouTube Video: [2023 2 01 Kilosort (Pachitariu)](https://www.youtube.com/watch?v=LTSmoACr918)

Obsidian:  ,  ,  ,  ,  ,  


<br>
### Install instructions

1. Clone Github Repo
2. Follow appropriate ReadMe instructions, 
	1. Install an¬†[Anaconda](https://www.anaconda.com/products/distribution)¬†distribution of Python. Note you might need to use an anaconda prompt if you did not add anaconda to the path.
	2. Open an anaconda prompt / command prompt which has¬†`conda`¬†for¬†**python 3**¬†in the path
	3. Create a new environment with¬†`conda create --name kilosort python=3.9`. Python 3.10 should work as well.
	4. To activate this new environment, run¬†`conda activate kilosort`
	5. To install kilosort and the GUI, run¬†`python -m pip install kilosort[gui]`
	7. Next, if the CPU version of pytorch was installed (will happen on Windows), remove it with¬†`pip uninstall torch`
	8. Then install the GPU version of pytorch¬†`conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia`

3. If Cuda, nvidia drivers not installed yet:

	If step 8 does not work, you need to make sure the NVIDIA driver for your GPU is installed (available¬†[here](https://www.nvidia.com/Download/index.aspx?lang=en-us)). You may also need to install the CUDA libraries for it, we recommend¬†[CUDA 11.8](https://developer.nvidia.com/cuda-11-8-0-download-archive).
	
	If pytorch installation still fails, follow the instructions¬†[here](https://pytorch.org/get-started/locally/)¬†to determine what version of pytorch to install. The Anaconda install is strongly recommended on Windows, and then choose the CUDA version that is supported by your GPU (newer GPUs may need newer CUDA versions > 10.2). For instance this command will install the 11.8 version on Linux and Windows (note the¬†`torchvision`¬†and¬†`torchaudio`¬†commands are removed because kilosort doesn't require them):
	
	`conda install pytorch pytorch-cuda=11.8 pynvml -c pytorch -c nvidia`
	
	This¬†[video](https://www.youtube.com/watch?v=gsixIQYvj3U)¬†has step-by-step installation instructions for NVIDIA drivers and pytorch in Windows (ignore the environment creation step with the .yml file, we have an environment already, to activate it use¬†`conda activate kilosort`).


4. Add missing packages `pip install ipykernel pandas`

5. Then downgrade numpy to be below 2.0.0 ([NumPy 2.0 compatibility ¬∑ Issue #722 ¬∑ MouseLand/Kilosort](https://github.com/MouseLand/Kilosort/issues/722)), `pip install --upgrade numpy<"2.0.0"`

<br>
### Pre-processing in kilosort


- ‚ùì kilosort code `artifact_threshold` $$\rightarrow$$ meaning?


| Step | Name                               | Purpose                                                                                                                               | Obsidian backlinks                                                                            |
| ---- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| 1    | Subtract mean                      | Remove the mean across time for each batch (2s)                                                                                       | f                                                                                             |
| 2    | Common average referencing (CAR)   | Subtract at each timepoint the median of the signal across all chanels                                                                | f                                                                                             |
| 3    | Temporal filtering                 | Something similar to butterworth high-pass fitler (cutoff = 300Hz), but done in frequency domain, see `fft_highpass` as faster on GPU | [[2023-10-30 Low-pass filter & high-pass filter]], [[2024-07-23 Spectral filtering in ephys]] |
| 4    | Channel whitening / Spatial filter | Remove correlations across channels, e.g. spikes from 100‚Äì1,000 Œºm away from the probe $$\rightarrow$$ done for nearest 32 channels     | Akseli docs:, separate doc: [[2024-09-17 Whitening data]]                                     |
| 5    | Drift correction                   | ...                                                                                                                                   |                                                                                               |



![image](images/Pasted image 20250307170720.png)

![image](images/Pasted image 20250307170731.png)
![image](images/Pasted image 20250307170743.png)
<br>
- Top: Original data: `amplifier.dat`, Bottom: `temp_wh.dat` (pre-processed raw data)
- ‚ö†Ô∏è in `C:\Users\FM\Documents\Akseli\Code\kilosort_pipeline\kilosort_plots.ipynb`, there is code with which you can save the pre-processed raw data, and then display in Neuroscope as done here

<br>
### Output files of `run_kilosort`

- üìö Text (partially) taken from documentation in ``Kilosort\kilosort\io.py``
- The following files (and some others not used in tutorial) will be saved in `results_dir`

<br>
<br>
### Meta data: `ops`

ops.npy : shape N/A
	`ops`
	Dictionary containing a number of state variables saved throughout
	the sorting process (see `run_kilosort`). We recommend loading with
	`kilosort.io.load_ops`.


<br>
<br>
### Channel map

channel_map.npy : shape (n_channels,)
	`chan_map`
	Same as probe['chanMap']. Integer indices into rows of binary file
	that map the data to the contacts listed in the probe file.

<br>
<br>
### Spike Times
spike_times.npy : shape (n_spikes,)
	`st`
	Sample index of the waveform peak for each spike.


<br>
<br>
### Templates vs clusters
- üìö See in `def save_to_phy` (in `io.py`) Note that 'template' here does not refer to the universal or learned templates used for spike detection, as it did in some past versions of Kilosort. Instead, it refers to the average spike waveform (after whitening, filtering, and drift correction) for all spikes assigned to each cluster, which are template-like in shape. We use the term 'template' anyway for this section because that is how they are treated in Phy. Elsewhere in the Kilosort4 code, we would refer to these as 'clusters.'

![image](images/Pasted image 20250307233457.png)
<br>


| variable in kilosort | filename saved for phy          | variable in phy                      | Plot above | Shape                         |
| -------------------- | ------------------------------- | ------------------------------------ | ---------- | ----------------------------- |
| spike_clusters       | 'spike_templates.npy'           | spike_templates/spike_clusters       | right      | (n_spikes,)                   |
| spike_clusters       | 'spike_clusters.npy'            | spike_templates/spike_clusters       | right      | (n_spikes,)                   |
| spike_templates      | 'spike_detection_templates.npy' |                                      | right?     | ???                           |
| templates            | 'templates.npy'                 | ``Bunch(data)`` in `class TraceView` | left?      | (n_templates, nt, n_channels) |


spike_clusters.npy : shape (n_spikes,)
	`clu`
	For each spike, integer indicating which template it was assigned to.
- ‚ùå spike_clusters & spike_templates are the same $$\rightarrow$$ see  


templates.npy : shape (n_templates, nt, n_channels $$\rightarrow$$  $$i, t, j$$)
	Full time x channels template shapes.
	- e.g. (n_templates, nt, n_channels) = (396, 62, 384)
		- where nt = number of time points in each template (template length: $$5ms$$)


![image](images/Pasted image 20241101221902.png)

- example of 3 different templates, for all nt (x-axis) and all channels (colours)


- ‚ö†Ô∏è `Templates (n_templates, nt, n_channels)` are averaged waveforms (over $$5ms$$ with 60 nt) for all the neurons detected within the same cluster. 


<br>
<br>
### Chan best


- `chan_best` is a list indexed by `n_templates` that for each template describes which channel has the most representative sum of detected waveforms similar to the template waveform
- ‚ùì `chan_best = chan_map[chan_best]`

`chan_best`
- shape (n_templates) 

```python
templates = ¬†np.load(results_dir / 'templates.npy')
chan_best = (templates**2).sum(axis=1).argmax(axis=-1)
```
- $$i$$ - templates, $$t$$ - time points 
- `.sum(axis=1)` $$\rightarrow$$ summing over time points (axis=1)


$$\text{sum\_squared}[i,j] = \sum_{t=1}^{62} (\text{templates}[i,t,j])^2 
$$

- Then apply `.argmax(axis=-1)` $$\rightarrow$$ index of the maximum value along **`axis=-1`**, which corresponds to the channels $$j$$ 

$$\text{chan\_best}[i]=\text{argmax}‚Äã(\text{sum\_squared}[i,j])
$$



![image](images/Pasted image 20241218110113.png)
<br>


- ‚ö†Ô∏è `clu` is an integer indicating, for each spike, which template the spike is assigned to. By passing `chan_best[clu]`, we get the channel number for each spike, since it's most likely that the spike originated from the channel where its associated template is most likely. 



![image](images/Pasted image 20241218110148.png)



<br>


<br>
<br>
### Firing rate activity

spike_clusters.npy : shape (n_spikes,)
	`clu`
	For each spike, integer indicating which template it was assigned to.


```python
clu = np.load(results_dir / 'spike_clusters.npy')
firing_rates = np.unique(clu, return_counts=True)[1] * 30000 / st.max()
```

- ‚ö†Ô∏è $$\text{firing\_rate} = \frac{\text{spike count}}{T} \times f_{s}$$ 
- ( )

```python
unique_elements, counts = np.unique([1, 2, 2, 2, 4, 4], return_counts=True)
# Output
Unique elements: [1 2 4]
Counts: [1 3 2]
```

`clu`: shape(n_spikes) $$\rightarrow$$ ``firing_rates``: shape(n_templates)

![image](images/Pasted image 20241101234233.png)

<br>
<br>
### Amplitude
amplitudes.npy : shape (n_spikes,)
	`amplitudes`
	Per-spike amplitudes, computed as the L2 norm of the PC features
	for each spike.

cluster_Amplitude.tsv : shape (n_templates,)
	`camps`
	Per-template amplitudes, computed as the L2 norm of the template.

- ‚ùì no access to the voltage amplitude???


<br>
<br>
### Spike positions

#todo 
- [ ] understand further

spike_position.npy : shape (n_spikes,2)
- ,2 $$\rightarrow$$ xy position $$\rightarrow$$ in Margot code then saved for each `clu` as separate `PosX`, `PosY`
<br>

- üìö `C:\Users\FM\anaconda3\envs\kilosort\Lib\site-packages\kilosort\postprocessing.py`: ![image](images/Pasted image 20250305181610.png)

![image](images/Pasted image 20250305180637.png)



<br>
<br>
### Contamination percentage: MUA vs Good units 

cluster_ContamPct.tsv : shape (n_clusters,)
	`contam_pct`
	"Contamination rate for each template, computed as fraction of refractory
	period violations relative to expectation based on a Poisson process."


<br>


|                   | Criterion 1                                                                                            | Criterion 2                                                                                                           |
| ----------------- | ------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |
| **Description**   | ratio of refractory coincidences ($$n_k$$) versus coincidences in other bins (e.g. "shoulders")          | probability $$P_k$$ that $$n_k$$ spikes or less would be observed from a Poisson process with rate $$\lambda_{k}= (2k+1)R$$ |
| **Limitation**    | if firing rate of unit is low $$\rightarrow$$ few refractory coincidences may be observed just by chance |                                                                                                                       |
| **Criteria**      | $$Q_{12}$$ $$\rightarrow  \min(\frac{Q_i}{\text{min}(\text{Q00}, \text{Q01})})$$                           | $$R_{12}$$ $$\rightarrow \min(P_k)$$ for different windows                                                                |
| **ACG threshold** | $$Q_{12} < 0.2$$                                                                                         | $$R_{12} < 0.2$$                                                                                                        |
| **CCG threshold** | $$Q_{12} < 0.25$$                                                                                        | $$R_{12} < 0.05$$                                                                                                       |



- ‚ùå $$Q_{12}, R_{12}$$ in our current kilosort version were mixed up in terms of variable names $$\rightarrow$$ in a new issue they will update this
- ‚ùì Later double check whether still $$Q$$ corresponds to criterion 1, and $$R$$ corresponds to criterion 2


- üìö The different thresholds for ACG and CCG have to do with the function of these decisions: for the ACG, we want small contamination rates $$R_{12}$$ because this indicates a well-isolated neuron, whereas for the CCG we want to prevent clusters from being split if their contamination rate $$R_{12}$$ is indicative of a relationship between these two clusters. This is similar for $$Q12$$


- ‚ö†Ô∏è For computing the CCG, ACG $$\rightarrow$$ Bin size =1ms, window ($$\delta t$$) = $$\pm 0.5s$$
- üìö Code from: `"C:\Users\FM\anaconda3\envs\kilosort\Lib\site-packages\kilosort\CCG.py"`
![image](images/Pasted image 20250307171018.png)
![image](images/Pasted image 20250307171042.png)

![image](images/Pasted image 20250307171058.png)
<br>


<br>
<br>
### Principal components 

- `pc_features` $$\rightarrow$$ shape(n_spikes, n_pcs, nearest_chans)
	- pc_features.npy
	- Tensor of pc features as returned by `template_matching.extract`,
	- ‚ùå Earlier in kilosort code the shape of `tF` is `(n_spikes, nearest_chans, n_pcs)`, but later they swap the last 2 dimensions as Phy expects this ordering
- `pc_features_ind` $$\rightarrow$$ shape(n_clusters, nearest_chans)
	- pc_features_ind.npy
	- Channel indices associated with the data present in tF for each cluster

- ‚ùì weird that they adjust shape of `tF` $$\rightarrow$$ in `class FeatureView` in `phy\cluster\views\feature.py`, they define shape of Bunch(data) as `(n_spikes, n_channels, n_features)`, and use that for constructing the FeatureView plots